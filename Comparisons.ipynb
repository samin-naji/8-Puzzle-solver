{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Comparison of Search Methods and Reinforcement Learning Methods\n",
        "\n",
        "## 1.1 Introduction\n",
        "\n",
        "The Eight-Puzzle problem can be solved using both **classical search algorithms** and **reinforcement learning (RL) methods**. In this project, we implemented and evaluated three approaches:\n",
        "\n",
        "* **A*** search with two heuristics (Misplaced Tiles and Manhattan Distance)\n",
        "* **Tabular Q-Learning**\n",
        "* **Deep Q-Network (DQN)**\n",
        "\n",
        "This section compares these methods in terms of their assumptions, computational cost, and solution quality, and analyzes their suitability for this problem.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Example Paths: A* vs Q-Learning Policy\n",
        "\n",
        "To ensure a fair comparison, all methods were tested on the same initial state:\n",
        "\n",
        "```\n",
        "Initial State: (1, 3, 6,\n",
        "                5, 0, 2,\n",
        "                4, 7, 8)\n",
        "```\n",
        "\n",
        "### **A* Solution Path (h1 and h2)**\n",
        "\n",
        "Both heuristics produced the same optimal path:\n",
        "\n",
        "```\n",
        "R U L D L D R R\n",
        "```\n",
        "\n",
        "* Path length: 8\n",
        "* Optimality: Guaranteed optimal\n",
        "\n",
        "### **Q-Learning Solution Path**\n",
        "\n",
        "The learned Q-Learning policy produced the following example path:\n",
        "\n",
        "```\n",
        "R U L D L D R R\n",
        "```\n",
        "\n",
        "* Path length: 8\n",
        "* Matches the optimal A* solution\n",
        "\n",
        "This shows that Q-Learning was able to learn an optimal policy for this specific problem instance.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.3 Quantitative Comparison of Results\n",
        "\n",
        "### **Performance Metrics**\n",
        "\n",
        "| Method                  | Path Length | Expanded Nodes | Execution Time | Success Rate |\n",
        "| ----------------------- | ----------- | -------------- | -------------- | ------------ |\n",
        "| A* (Misplaced Tiles)    | 8           | 17             | ~0.000 s       | 100%         |\n",
        "| A* (Manhattan Distance) | 8           | 13             | ~0.000 s       | 100%         |\n",
        "| Q-Learning              | 8 (avg)     | 301,388        | 13.50 s        | 100%         |\n",
        "| DQN                     | 8 (avg)     | 400            | 0.07 s         | 100%         |\n",
        "\n",
        "---\n",
        "\n",
        "## 1.4 Comparison of A* with Q-Learning and DQN\n",
        "\n",
        "### 1. Need for an Environment Model (Transition Model)\n",
        "\n",
        "* **A*** requires a **complete and accurate transition model**.\n",
        "\n",
        "  * It must explicitly know how actions change the state.\n",
        "  * Successor generation is hard-coded.\n",
        "* **Q-Learning and DQN** do **not require an explicit model**.\n",
        "\n",
        "  * They learn solely through interaction with the environment.\n",
        "  * This makes them model-free methods.\n",
        "\n",
        "**Conclusion:**\n",
        "A* is model-dependent, while RL methods are model-free.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Computational Cost and Number of Visited States\n",
        "\n",
        "* **A***:\n",
        "\n",
        "  * Extremely efficient for this problem.\n",
        "  * Expanded as few as **13 nodes** using Manhattan distance.\n",
        "  * Execution time was negligible.\n",
        "* **Q-Learning**:\n",
        "\n",
        "  * Required **over 300,000 expanded nodes** during training.\n",
        "  * Training time was significantly longer (13.5 seconds).\n",
        "* **DQN**:\n",
        "\n",
        "  * Required far fewer expanded nodes during evaluation.\n",
        "  * Training is more expensive than A*, but evaluation is fast.\n",
        "  * More efficient than tabular Q-Learning due to generalization.\n",
        "\n",
        "**Conclusion:**\n",
        "A* is vastly more efficient for small, well-defined problems like the Eight-Puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Quality of the Obtained Paths (Optimality and Length)\n",
        "\n",
        "* **A***:\n",
        "\n",
        "  * Guarantees optimal paths when using admissible heuristics.\n",
        "  * Always returns the shortest solution.\n",
        "* **Q-Learning**:\n",
        "\n",
        "  * Can learn optimal paths, but without guarantees.\n",
        "  * Convergence depends on exploration and training time.\n",
        "* **DQN**:\n",
        "\n",
        "  * Produces near-optimal paths.\n",
        "  * Does not guarantee optimality due to function approximation.\n",
        "\n",
        "In this experiment, all methods achieved a **path length of 8**, indicating optimal or near-optimal performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.5 Advantages and Disadvantages Summary\n",
        "\n",
        "### **A***\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "* Guaranteed optimal solution\n",
        "* Very low computational cost\n",
        "* Deterministic and interpretable\n",
        "* Ideal for small, discrete problems\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "* Requires a known transition model\n",
        "* Does not generalize to unseen problems\n",
        "* Not suitable for large or continuous state spaces\n",
        "\n",
        "---\n",
        "\n",
        "### **Q-Learning**\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "* Model-free\n",
        "* Simple to implement\n",
        "* Learns optimal policies given enough exploration\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "* Large memory requirements\n",
        "* Slow convergence\n",
        "* Poor scalability\n",
        "\n",
        "---\n",
        "\n",
        "### **DQN**\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "* Generalizes across states\n",
        "* Scales to larger state spaces\n",
        "* Efficient inference once trained\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "* More complex\n",
        "* Training instability\n",
        "* No optimality guarantee\n",
        "* Overkill for small problems"
      ],
      "metadata": {
        "id": "tDuPmTjeB6Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Comparison of Q-Learning and Deep Q-Network (DQN)\n",
        "\n",
        "### 2.1 Overview of the Two Approaches\n",
        "\n",
        "Q-Learning and Deep Q-Networks (DQN) are both value-based reinforcement learning algorithms that aim to learn an optimal action-value function Q(s, a). However, they differ fundamentally in how this function is represented.\n",
        "\n",
        "* **Q-Learning** uses a **tabular representation**, storing explicit Q-values for each state-action pair.\n",
        "* **DQN** replaces the Q-table with a **neural network**, which approximates Q(s, a) and generalizes across states.\n",
        "\n",
        "In this project, both methods were applied to the Eight-Puzzle problem using the same Gym environment and reward structure.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Advantages and Disadvantages in This Problem Setting\n",
        "\n",
        "The Eight-Puzzle has a **relatively small and discrete state space** compared to many modern reinforcement learning problems. This strongly affects the relative performance of the two algorithms.\n",
        "\n",
        "#### **Q-Learning**\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "* Simple and easy to implement\n",
        "* Guaranteed convergence (under standard assumptions)\n",
        "* Learns exact Q-values for visited states\n",
        "* Produces stable and deterministic policies\n",
        "* No function approximation error\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "* Requires storing a Q-value for every visited state\n",
        "* Does not generalize to unseen states\n",
        "* Becomes inefficient as the state space grows\n",
        "* Large number of expanded nodes during training\n",
        "\n",
        "#### **DQN**\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "* Can generalize across similar states\n",
        "* Does not require storing an explicit Q-table\n",
        "* Scales better to large or continuous state spaces\n",
        "* Faster inference once trained\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "* More complex implementation\n",
        "* Requires careful tuning of hyperparameters\n",
        "* Training instability due to function approximation\n",
        "* No guarantee of learning the optimal policy\n",
        "* Higher computational overhead for small problems\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Did DQN Show a Significant Advantage Over Q-Learning?\n",
        "\n",
        "In this particular problem, **DQN did not show a significant advantage over Q-Learning**.\n",
        "\n",
        "Both methods:\n",
        "\n",
        "* Achieved a **100% success rate**\n",
        "* Produced **near-optimal solution paths** (≈ 8 moves)\n",
        "* Solved the puzzle reliably\n",
        "\n",
        "However:\n",
        "\n",
        "* Q-Learning required **significantly more expanded nodes** during training.\n",
        "* DQN converged to a stable policy more quickly and executed solutions faster during evaluation.\n",
        "\n",
        "Despite this, the **final policy quality was similar**, and the additional complexity of DQN did not result in substantially better performance.\n",
        "\n",
        "**Reason:**\n",
        "The Eight-Puzzle has a **small, fully observable, discrete state space**, which is ideal for tabular methods. In such settings, Q-Learning is sufficient and often preferable.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4 When Would DQN Outperform Q-Learning?\n",
        "\n",
        "DQN is expected to perform significantly better than Q-Learning in problems with:\n",
        "\n",
        "* Very **large state spaces**\n",
        "* **Continuous or high-dimensional observations**\n",
        "* **Image-based inputs** (e.g., Atari games)\n",
        "* Environments where **generalization** is required\n",
        "* Tasks where storing a Q-table is infeasible\n",
        "\n",
        "Examples include:\n",
        "\n",
        "* Robotic control\n",
        "* Autonomous driving\n",
        "* Video game environments\n",
        "* Continuous control tasks\n",
        "\n",
        "In these cases, tabular Q-Learning becomes impractical, while DQN can learn useful representations using neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.5 Final Method Selection\n",
        "\n",
        "If only **one method** were to be chosen for solving the Eight-Puzzle problem, **Q-Learning** would be the preferred choice.\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "* The state space is small and discrete\n",
        "* Q-Learning is simpler and more transparent\n",
        "* It provides stable convergence and exact value estimates\n",
        "* No need for neural networks or replay buffers\n",
        "* Lower implementation and debugging complexity\n",
        "\n",
        "DQN, while powerful, introduces unnecessary complexity for this specific task and does not offer a meaningful performance advantage.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.6 Summary Table\n",
        "\n",
        "| Criterion                 | Q-Learning      | DQN                     |\n",
        "| ------------------------- | --------------- | ----------------------- |\n",
        "| State space suitability   | Small, discrete | Large, high-dimensional |\n",
        "| Implementation complexity | Low             | High                    |\n",
        "| Memory usage              | High (Q-table)  | Low (network weights)   |\n",
        "| Generalization            | None            | Yes                     |\n",
        "| Training stability        | High            | Moderate                |\n",
        "| Optimality guarantee      | Yes (tabular)   | No                      |\n",
        "| Best choice for 8-Puzzle  | ✅               | ❌                       |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "For small, deterministic problems like the Eight-Puzzle, tabular Q-Learning remains a strong and reliable choice. DQN becomes advantageous only when the problem scale exceeds the limits of tabular methods."
      ],
      "metadata": {
        "id": "3eiarNzVHOUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Overall Conclusion and Method Selection\n",
        "\n",
        "If only **one method** had to be chosen to solve the Eight-Puzzle problem, **A*** would be the preferred choice.\n",
        "\n",
        "### **Reasoning**\n",
        "\n",
        "* The problem has a **small, discrete, fully known state space**\n",
        "* An admissible heuristic (Manhattan distance) is available\n",
        "* A* finds the **optimal solution with minimal computation**\n",
        "* Reinforcement learning methods require significantly more computation to reach the same result\n",
        "\n",
        "### **Final Assessment**\n",
        "\n",
        "* **A*** is the best practical solution for this problem.\n",
        "* **Q-Learning** demonstrates learning capability but is inefficient.\n",
        "* **DQN** is powerful but unnecessary for such a small domain.\n",
        "\n",
        "RL methods become advantageous only when:\n",
        "\n",
        "* The transition model is unknown\n",
        "* The state space is very large or continuous\n",
        "* Generalization across many tasks is required\n",
        "\n",
        "---\n",
        "\n",
        "## **Final Remark**\n",
        "\n",
        "This comparison highlights that the choice of algorithm should be guided by the **problem structure**, not just algorithmic sophistication. While reinforcement learning is powerful, classical search methods remain superior for small, deterministic planning problems such as the Eight-Puzzle."
      ],
      "metadata": {
        "id": "DvY_D31ADnvB"
      }
    }
  ]
}