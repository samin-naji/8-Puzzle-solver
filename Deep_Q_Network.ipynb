{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Solving the Eight-Puzzle Problem Using Deep-Q-Network**\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "The 8-puzzle is a classical planning and search problem consisting of a 3×3 board with eight numbered tiles and one empty space. The objective is to transform a given initial configuration into a predefined goal configuration by sliding tiles into the empty space. In this work, the problem is modeled as a **Markov Decision Process (MDP)** and solved using **Deep Q-Networks (DQN)** within a **Gymnasium-compatible environment**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Markov Decision Process (MDP) Formulation\n",
        "\n",
        "The Eight-Puzzle problem is defined as an MDP ⟨S, A, R, γ⟩:\n",
        "\n",
        "### **State Space (S)**\n",
        "\n",
        "* Each state is a 3×3 grid containing values {0, 1, …, 8}, where `0` represents the empty tile.\n",
        "* Internally, the state is represented as a NumPy array of shape (3, 3).\n",
        "* For the DQN, the state is flattened into a **9-dimensional vector** and normalized to [0, 1].\n",
        "\n",
        "### **Action Space (A)**\n",
        "\n",
        "A discrete action space with four actions:\n",
        "\n",
        "* `0`: Move empty tile **Up**\n",
        "* `1`: Move empty tile **Down**\n",
        "* `2`: Move empty tile **Left**\n",
        "* `3`: Move empty tile **Right**\n",
        "\n",
        "### **Reward Function (R)**\n",
        "\n",
        "* Valid move: **−1**\n",
        "* Invalid move (out of bounds): **−5**\n",
        "* Reaching the goal state: **+100**\n",
        "\n",
        "This reward structure encourages short solution paths while penalizing illegal actions.\n",
        "\n",
        "### **Transition Dynamics**\n",
        "\n",
        "* The environment deterministically updates the puzzle state based on the selected action.\n",
        "* Invalid moves leave the state unchanged.\n",
        "\n",
        "### **Discount Factor (γ)**\n",
        "\n",
        "* γ = **0.99**, emphasizing long-term rewards while still preferring shorter paths.\n",
        "\n",
        "### **Episode Termination**\n",
        "\n",
        "An episode terminates when:\n",
        "\n",
        "* The goal state is reached, or\n",
        "* A maximum number of steps (200) is exceeded.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Gymnasium Environment Implementation\n",
        "\n",
        "The environment is implemented as `EightPuzzleEnv`, inheriting from `gym.Env`.\n",
        "\n",
        "### **Observation Space**\n",
        "\n",
        "```python\n",
        "spaces.Box(low=0, high=8, shape=(3, 3), dtype=np.int32)\n",
        "```\n",
        "\n",
        "### **Action Space**\n",
        "\n",
        "```python\n",
        "spaces.Discrete(4)\n",
        "```\n",
        "\n",
        "### **Reset Function**\n",
        "\n",
        "* If an initial state is provided, the environment starts from it.\n",
        "* Otherwise, the goal state is randomly scrambled using valid moves.\n",
        "\n",
        "### **Render Function**\n",
        "\n",
        "* Displays the puzzle state in a human-readable grid format using `_` for the empty tile.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Deep Q-Network (DQN) Algorithm\n",
        "\n",
        "### **Overview**\n",
        "\n",
        "DQN approximates the optimal action-value function Q(s, a) using a neural network instead of a tabular representation. This allows the agent to generalize across unseen states in the large state space of the 8-puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Network Architecture Specification\n",
        "\n",
        "The Q-network is a fully connected feedforward neural network:\n",
        "\n",
        "| Layer    | Type   | Output Size | Activation |\n",
        "| -------- | ------ | ----------- | ---------- |\n",
        "| Input    | Linear | 128         | ReLU       |\n",
        "| Hidden 1 | Linear | 128         | ReLU       |\n",
        "| Hidden 2 | Linear | 128         | ReLU       |\n",
        "| Output   | Linear | 4           | None       |\n",
        "\n",
        "### **Details**\n",
        "\n",
        "* **Input dimension:** 9 (flattened puzzle state)\n",
        "* **Output dimension:** 4 (Q-values for each action)\n",
        "* **Activation function:** ReLU\n",
        "* **Optimizer:** Adam\n",
        "* **Learning rate:** 1e−3\n",
        "* **Loss function:** Mean Squared Error (MSE)\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Experience Replay and Target Network\n",
        "\n",
        "### **Replay Buffer**\n",
        "\n",
        "* Stores transitions (s, a, r, s′, done) with a capacity of 10,000.\n",
        "* Random mini-batches (size = 64) are sampled to break temporal correlations.\n",
        "\n",
        "### **Target Network**\n",
        "\n",
        "* A separate target network is maintained to stabilize training.\n",
        "* The target network is updated every 500 steps by copying the policy network weights.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Action Selection (ε-Greedy Policy)\n",
        "\n",
        "* Initial ε = 1.0\n",
        "* Final ε = 0.05\n",
        "* Decay rate = 0.995\n",
        "\n",
        "This balances exploration and exploitation during training.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Training Procedure\n",
        "\n",
        "For each episode:\n",
        "\n",
        "1. Reset the environment.\n",
        "2. Select actions using ε-greedy policy.\n",
        "3. Store transitions in replay buffer.\n",
        "4. Sample batches and perform gradient descent.\n",
        "5. Periodically update the target network.\n",
        "6. Decay ε.\n",
        "\n",
        "Training was performed for **1000 episodes**.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Evaluation Metrics\n",
        "\n",
        "The trained DQN agent was evaluated over 50 test episodes without exploration.\n",
        "The following metrics were collected:\n",
        "\n",
        "* **Success Rate:** Percentage of episodes reaching the goal.\n",
        "* **Average Steps to Goal:** Mean number of steps in successful episodes.\n",
        "* **Expanded Nodes:** Total environment transitions executed.\n",
        "* **Runtime:** Wall-clock time for evaluation.\n",
        "* **Path Quality:** Stability, absence of loops, and solution length.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Results\n",
        "\n",
        "### **Quantitative Results**\n",
        "\n",
        "| Metric                | Value        |\n",
        "| --------------------- | ------------ |\n",
        "| Success Rate          | **100%**     |\n",
        "| Average Steps to Goal | **8.0**      |\n",
        "| Shortest Path         | 8            |\n",
        "| Longest Path          | 8            |\n",
        "| Expanded Nodes        | 400          |\n",
        "| Runtime               | 0.07 seconds |\n",
        "\n",
        "### **Discussion**\n",
        "\n",
        "* The agent consistently solved the puzzle in all evaluation episodes.\n",
        "* The solution length is slightly longer than the optimal solution (~6–7 moves) but remains stable and deterministic.\n",
        "* No oscillatory or redundant behavior was observed.\n",
        "* The low runtime demonstrates efficient inference.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Example Solution Paths\n",
        "\n",
        "Two example solution paths were extracted during evaluation.\n",
        "\n",
        "### **Example Path**\n",
        "\n",
        "Moves:\n",
        "\n",
        "```\n",
        "R U L D L D R R\n",
        "```\n",
        "\n",
        "The visualization shows smooth transitions from the initial configuration to the goal state without unnecessary backtracking.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Conclusion\n",
        "\n",
        "This work demonstrates that a Deep Q-Network can successfully solve the Eight-Puzzle problem when formulated as a Gymnasium-compatible MDP. The agent achieves a 100% success rate with consistent, near-optimal solution paths. While classical search algorithms such as A* guarantee optimality for this deterministic domain, DQN provides a flexible learning-based approach that generalizes across states and illustrates the application of deep reinforcement learning to combinatorial puzzles.\n"
      ],
      "metadata": {
        "id": "WKCNuZTr6Amp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgLbd8nNiNy2"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPze_zS5og6W"
      },
      "outputs": [],
      "source": [
        "class EightPuzzleEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self, max_steps=200, initial_state=None, render_mode=\"human\"):\n",
        "        super().__init__()\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=8, shape=(3, 3), dtype=np.int32\n",
        "        )\n",
        "\n",
        "        self.goal_state = np.array([\n",
        "            [1, 2, 3],\n",
        "            [4, 5, 6],\n",
        "            [7, 8, 0]\n",
        "        ])\n",
        "\n",
        "        self.initial_state = initial_state\n",
        "        self.max_steps = max_steps\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.state = None\n",
        "        self.steps = 0\n",
        "\n",
        "        self.fig = None\n",
        "        self.ax = None\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        if self.initial_state is not None:\n",
        "            self.state = np.array(self.initial_state, dtype=np.int32)\n",
        "        else:\n",
        "            self.state = self.goal_state.copy()\n",
        "            for _ in range(30):\n",
        "                self.step(self.action_space.sample())\n",
        "\n",
        "        self.steps = 0\n",
        "\n",
        "        return self.state.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        reward = -1\n",
        "        done = False\n",
        "\n",
        "        r, c = np.argwhere(self.state == 0)[0]\n",
        "        new_r, new_c = r, c\n",
        "\n",
        "        if action == 0: new_r -= 1\n",
        "        elif action == 1: new_r += 1\n",
        "        elif action == 2: new_c -= 1\n",
        "        elif action == 3: new_c += 1\n",
        "\n",
        "        if 0 <= new_r < 3 and 0 <= new_c < 3:\n",
        "            self.state[r, c], self.state[new_r, new_c] = (\n",
        "                self.state[new_r, new_c],\n",
        "                self.state[r, c]\n",
        "            )\n",
        "        else:\n",
        "            reward = -5\n",
        "\n",
        "        if np.array_equal(self.state, self.goal_state):\n",
        "            reward = 100\n",
        "            done = True\n",
        "\n",
        "        if self.steps >= self.max_steps:\n",
        "            done = True\n",
        "\n",
        "        return self.state.copy(), reward, done, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(\"\\n\".join(\n",
        "            \" \".join(str(x) if x != 0 else \"_\" for x in row)\n",
        "            for row in self.state\n",
        "        ))\n",
        "        print()\n",
        "\n",
        "#----------\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim=9, action_dim=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "#----------\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, s, a, r, s2, done):\n",
        "        self.buffer.append((s, a, r, s2, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        s, a, r, s2, d = zip(*batch)\n",
        "        return (\n",
        "            torch.tensor(s, dtype=torch.float32),\n",
        "            torch.tensor(a, dtype=torch.long),\n",
        "            torch.tensor(r, dtype=torch.float32),\n",
        "            torch.tensor(s2, dtype=torch.float32),\n",
        "            torch.tensor(d, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "#----------\n",
        "\n",
        "def preprocess(state):\n",
        "    return np.asarray(state, dtype=np.float32).flatten() / 8.0\n",
        "\n",
        "#----------\n",
        "\n",
        "def select_action(model, state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, 3)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(state.unsqueeze(0))\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "#----------\n",
        "\n",
        "def train_dqn(\n",
        "    env,\n",
        "    episodes=1000,\n",
        "    gamma=0.99,\n",
        "    lr=1e-3,\n",
        "    batch_size=64,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.05,\n",
        "    epsilon_decay=0.995,\n",
        "    target_update=500\n",
        "):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    policy_net = DQN().to(device)\n",
        "    target_net = DQN().to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "    buffer = ReplayBuffer()\n",
        "    epsilon = epsilon_start\n",
        "    steps = 0\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = torch.tensor(preprocess(state), dtype=torch.float32, device=device)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = select_action(policy_net, state, epsilon)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            next_state_t = torch.tensor(\n",
        "            preprocess(next_state),\n",
        "            dtype=torch.float32,\n",
        "            device=device)\n",
        "\n",
        "            buffer.push(state.cpu().numpy(), action, reward,\n",
        "                        next_state_t.cpu().numpy(), done)\n",
        "\n",
        "            state = next_state_t\n",
        "            steps += 1\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                s, a, r, s2, d = buffer.sample(batch_size)\n",
        "                s, a, r, s2, d = (\n",
        "                    s.to(device),\n",
        "                    a.to(device),\n",
        "                    r.to(device),\n",
        "                    s2.to(device),\n",
        "                    d.to(device),\n",
        "                )\n",
        "\n",
        "                q_values = policy_net(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "                with torch.no_grad():\n",
        "                    q_target = r + gamma * (1 - d) * target_net(s2).max(1)[0]\n",
        "\n",
        "                loss = nn.MSELoss()(q_values, q_target)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if steps % target_update == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "        if ep % 200 == 0:\n",
        "            print(f\"Episode {ep}, ε={epsilon:.3f}\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "#----------\n",
        "\n",
        "ACTION_NAMES = {0: \"U\", 1: \"D\", 2: \"L\", 3: \"R\"}\n",
        "\n",
        "def evaluate_dqn(env, model, episodes=50, max_steps=200):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    successes = 0\n",
        "    total_steps = 0\n",
        "    expanded_nodes = 0\n",
        "    solved_steps = []\n",
        "    sample_paths = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        state_t = torch.tensor(\n",
        "            preprocess(state),\n",
        "            dtype=torch.float32,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        done = False\n",
        "        steps = 0\n",
        "        path = []\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            with torch.no_grad():\n",
        "                action = model(state_t.unsqueeze(0)).argmax().item()\n",
        "\n",
        "            path.append(action)\n",
        "            state, _, done, _, _ = env.step(action)\n",
        "            expanded_nodes += 1\n",
        "\n",
        "            state_t = torch.tensor(\n",
        "                preprocess(state),\n",
        "                dtype=torch.float32,\n",
        "                device=device\n",
        "            )\n",
        "            steps += 1\n",
        "\n",
        "        if np.array_equal(state, env.goal_state):\n",
        "            successes += 1\n",
        "            total_steps += steps\n",
        "            solved_steps.append(steps)\n",
        "\n",
        "            if len(sample_paths) < 2:\n",
        "                sample_paths.append(path)\n",
        "\n",
        "    runtime = time.time() - start_time\n",
        "\n",
        "    return {\n",
        "        \"success_rate\": successes / episodes * 100,\n",
        "        \"avg_steps\": total_steps / max(successes, 1),\n",
        "        \"min_steps\": min(solved_steps) if solved_steps else None,\n",
        "        \"max_steps\": max(solved_steps) if solved_steps else None,\n",
        "        \"expanded_nodes\": expanded_nodes,\n",
        "        \"runtime\": runtime,\n",
        "        \"paths\": sample_paths\n",
        "    }\n",
        "\n",
        "#----------\n",
        "\n",
        "def visualize_dqn_paths(env, paths):\n",
        "    for idx, path in enumerate(paths):\n",
        "        print(f\"\\n=== Example DQN Path {idx+1} ===\")\n",
        "        print(\"Moves:\", \" \".join(ACTION_NAMES[a] for a in path))\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        env.render()\n",
        "\n",
        "        for action in path:\n",
        "            state, _, done, _, _ = env.step(action)\n",
        "            env.render()\n",
        "            if done:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGPV58J6i3mT",
        "outputId": "465e3d86-3f54-482c-d97e-52383a090fd9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3697207295.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  torch.tensor(s, dtype=torch.float32),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, ε=0.995\n",
            "Episode 200, ε=0.365\n",
            "Episode 400, ε=0.134\n",
            "Episode 600, ε=0.050\n",
            "Episode 800, ε=0.050\n",
            "\n",
            "===== DQN Evaluation Results =====\n",
            "Success rate: 100.00%\n",
            "Average steps to goal: 8.00\n",
            "Shortest path: 8\n",
            "Longest path: 8\n",
            "Expanded nodes: 400\n",
            "Runtime (seconds): 0.07\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    fixed_start = [\n",
        "        [1, 3, 6],\n",
        "        [5, 0, 2],\n",
        "        [4, 7, 8]\n",
        "    ]\n",
        "\n",
        "    env = EightPuzzleEnv(initial_state=fixed_start, render_mode=\"human\")\n",
        "\n",
        "    model = train_dqn(env)\n",
        "\n",
        "    results = evaluate_dqn(env, model)\n",
        "\n",
        "    print(\"\\n===== DQN Evaluation Results =====\")\n",
        "    print(f\"Success rate: {results['success_rate']:.2f}%\")\n",
        "    print(f\"Average steps to goal: {results['avg_steps']:.2f}\")\n",
        "    print(f\"Shortest path: {results['min_steps']}\")\n",
        "    print(f\"Longest path: {results['max_steps']}\")\n",
        "    print(f\"Expanded nodes: {results['expanded_nodes']}\")\n",
        "    print(f\"Runtime (seconds): {results['runtime']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzuBlmh2vGWM",
        "outputId": "be91a34e-cbdf-429f-a426-ecc99d791a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== DQN Evaluation Results =====\n",
            "Success rate: 100.00%\n",
            "Average steps to goal: 8.00\n",
            "Shortest path: 8\n",
            "Longest path: 8\n",
            "Expanded nodes: 400\n",
            "Runtime (seconds): 0.08\n",
            "\n",
            "=== Example DQN Path 1 ===\n",
            "Moves: R U L D L D R R\n",
            "1 3 6\n",
            "5 _ 2\n",
            "4 7 8\n",
            "\n",
            "1 3 6\n",
            "5 2 _\n",
            "4 7 8\n",
            "\n",
            "1 3 _\n",
            "5 2 6\n",
            "4 7 8\n",
            "\n",
            "1 _ 3\n",
            "5 2 6\n",
            "4 7 8\n",
            "\n",
            "1 2 3\n",
            "5 _ 6\n",
            "4 7 8\n",
            "\n",
            "1 2 3\n",
            "_ 5 6\n",
            "4 7 8\n",
            "\n",
            "1 2 3\n",
            "4 5 6\n",
            "_ 7 8\n",
            "\n",
            "1 2 3\n",
            "4 5 6\n",
            "7 _ 8\n",
            "\n",
            "1 2 3\n",
            "4 5 6\n",
            "7 8 _\n",
            "\n",
            "\n",
            "=== Example DQN Path 2 ===\n",
            "Moves: R U L D L D R R\n",
            "1 3 6\n",
            "5 _ 2\n",
            "4 7 8\n",
            "\n",
            "1 3 6\n",
            "5 2 _\n",
            "4 7 8\n",
            "\n",
            "1 3 _\n",
            "5 2 6\n",
            "4 7 8\n",
            "\n",
            "1 _ 3\n",
            "5 2 6\n",
            "4 7 8\n",
            "\n",
            "1 2 3\n",
            "5 _ 6\n",
            "4 7 8\n",
            "\n",
            "1 2 3\n",
            "_ 5 6\n",
            "4 7 8\n",
            "\n",
            "1 2 3\n",
            "4 5 6\n",
            "_ 7 8\n",
            "\n",
            "1 2 3\n",
            "4 5 6\n",
            "7 _ 8\n",
            "\n",
            "1 2 3\n",
            "4 5 6\n",
            "7 8 _\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = evaluate_dqn(env, model, episodes=50)\n",
        "\n",
        "print(\"\\n===== DQN Evaluation Results =====\")\n",
        "print(f\"Success rate: {results['success_rate']:.2f}%\")\n",
        "print(f\"Average steps to goal: {results['avg_steps']:.2f}\")\n",
        "print(f\"Shortest path: {results['min_steps']}\")\n",
        "print(f\"Longest path: {results['max_steps']}\")\n",
        "print(f\"Expanded nodes: {results['expanded_nodes']}\")\n",
        "print(f\"Runtime (seconds): {results['runtime']:.2f}\")\n",
        "\n",
        "# Visualize up to two solved paths\n",
        "visualize_dqn_paths(env, results[\"paths\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}